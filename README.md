# ğŸ§  C3NLP

**C3NLP** Ã© um sistema de **GeraÃ§Ã£o de Respostas Contextuais** utilizando **RAG (Retrieval-Augmented Generation)**, construÃ­do com:

- **[Streamlit](https://streamlit.io/)** â†’ Interface web interativa para exibiÃ§Ã£o e upload de arquivos.  
- **[LangChain](https://www.langchain.com/)** â†’ OrquestraÃ§Ã£o de LLMs, memÃ³ria de conversas e fluxo de RAG.  
- **[Ollama](https://ollama.com/)** â†’ ExecuÃ§Ã£o de LLMs localmente, como `LLaMA 3` e `DeepSeek`.  

---

## ğŸ“š O que cada tecnologia faz

- **LangChain**:  
  Framework para criar pipelines de LLMs com suporte a RAG, embeddings, agentes e memÃ³ria de conversas.

- **Ollama**:  
  Plataforma para executar modelos de LLM localmente, gerenciando downloads e execuÃ§Ã£o com aceleraÃ§Ã£o via GPU.

- **Streamlit**:  
  Framework Python para criar aplicaÃ§Ãµes web rÃ¡pidas e interativas, perfeito para dashboards e protÃ³tipos de IA.

---

## ğŸ¤– Modelos Utilizados

Atualmente, o projeto suporta e utiliza os seguintes modelos:

- **LLMs (via Ollama)**:
  - `llama3`
  - `deepseek-r1:14b`  

- **Embeddings e Rerankers** (para RAG):
  - `BAAI/bge-small-en-v1.5` â†’ GeraÃ§Ã£o de embeddings  
  - `ms-marco-MiniLM-L-12-v2` â†’ Re-ranqueamento semÃ¢ntico  

---

## âš™ï¸ PrÃ©-processamento de Documentos

1. **Upload do arquivo** (`.pdf`, `.txt`, `.md`)  
2. **ExtraÃ§Ã£o de conteÃºdo**:
   - PDF â†’ Lido com `pypdfium2`  
   - TXT/MD â†’ Lido e decodificado em UTF-8  
3. **Chunking**:
   - **Tamanho:** `2048` tokens  
   - **SobreposiÃ§Ã£o:** `128` tokens  
4. **GeraÃ§Ã£o de Embeddings**  
5. **IndexaÃ§Ã£o para RAG** com recuperaÃ§Ã£o BM25 + semÃ¢ntica  

---

## ğŸ“¦ Requisitos

- **Python 3.13.**    
- **[Ollama](https://ollama.com/)** instalado e rodando localmente  
- **Modelos LLM baixados localmente**, por exemplo:
  ```bash
  ollama pull llama3
  ollama pull deepseek-r1:14b
  ```

---

## ğŸš€ Fluxo de ExecuÃ§Ã£o do Chatbot

```mermaid
flowchart TD
    A[Upload de Arquivo] --> B[load_uploaded_file]
    B --> C[Arquivo carregado]
    C --> D[UsuÃ¡rio faz pergunta]
    D --> E[ask]
    E --> F[_ask_model]
    F --> G{workflow.stream}

    G -->|SourcesEvent| H[Documentos recuperados]
    G -->|ChunkEvent| I[Streaming de resposta]
    G -->|FinalAnswerEvent| J[Resposta final]

    J --> K[Atualiza histÃ³rico e retorna]



---

## ğŸ–¥ï¸ Como Rodar o Projeto

### 1ï¸âƒ£ Clone o repositÃ³rio
```bash
git clone git@github.com:Guilherme-Eduardo/C3NLP.git
cd C3NLP
```

### 2ï¸âƒ£ Crie um ambiente virtual e instale as dependÃªncias
```bash
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
.venv\Scripts\activate     # Windows

pip install -r requirements.txt
```

### 3ï¸âƒ£ Execute a aplicaÃ§Ã£o
```bash
streamlit run C3NLP/app.py
```

Acesse no navegador: **http://localhost:8501**

---

## ğŸ³ Rodando com Docker

...